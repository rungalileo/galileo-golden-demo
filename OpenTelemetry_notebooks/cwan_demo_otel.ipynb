{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finance Agent Demo: OpenTelemetry + Galileo + AWS Bedrock\n",
        "\n",
        "This notebook demonstrates how to integrate **OpenTelemetry** with **Galileo** for comprehensive observability of agentic LLM workflows using **AWS Bedrock**.\n",
        "\n",
        "## üìö OpenTelemetry Basics\n",
        "\n",
        "### What is OpenTelemetry?\n",
        "\n",
        "**OpenTelemetry (OTel)** is an open-source observability framework that provides a unified way to collect, process, and export telemetry data (traces, metrics, logs) from your applications.\n",
        "\n",
        "**Key Concepts**:\n",
        "- **Traces**: Records of operations that show the flow of requests through your system\n",
        "- **Spans**: Individual operations within a trace (e.g., an LLM call, a tool invocation)\n",
        "- **Tracer Provider**: Manages span creation and exports traces to backends\n",
        "- **Exporters**: Send traces to observability platforms (like Galileo)\n",
        "\n",
        "### Why Use OpenTelemetry?\n",
        "\n",
        "1. **Standardization**: Industry-standard format for observability data\n",
        "2. **Vendor Neutral**: Works with any observability platform (Galileo, Datadog, New Relic, etc.)\n",
        "3. **Automatic Instrumentation**: Libraries can automatically create spans for common operations\n",
        "4. **Rich Metadata**: Captures detailed information about operations (timing, attributes, context)\n",
        "\n",
        "### What You Need to Know\n",
        "\n",
        "**Core Components**:\n",
        "- **Tracer Provider**: Creates and manages spans\n",
        "- **Span Processor**: Processes spans (batches, filters, exports)\n",
        "- **Exporter**: Sends traces to your backend (Galileo in this case)\n",
        "- **Instrumentation**: Automatic or manual code that creates spans\n",
        "\n",
        "**Key Terms**:\n",
        "- **Trace**: A complete request flow (e.g., one user query through the entire agent workflow)\n",
        "- **Span**: A single operation (e.g., one LLM call, one tool execution)\n",
        "- **Attributes**: Key-value pairs attached to spans (e.g., `ticker=\"AAPL\"`, `temperature=0.1`)\n",
        "- **OTLP**: OpenTelemetry Protocol - the standard format for sending telemetry data\n",
        "\n",
        "**How It Works**:\n",
        "1. Your code (or automatic instrumentation) creates spans\n",
        "2. Spans are collected by the tracer provider\n",
        "3. Span processor batches and processes spans\n",
        "4. Exporter sends spans to Galileo via OTLP\n",
        "5. Galileo visualizes traces in its console\n",
        "\n",
        "### Trace Structure Example\n",
        "\n",
        "When you run a query like \"What's the price of AAPL?\", here's how traces are structured:\n",
        "\n",
        "```\n",
        "Trace (one user query)\n",
        "‚îú‚îÄ‚îÄ Span: finance_agent_query (root)\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ Span: LangGraph (workflow)\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ Span: agent (LLM decision)\n",
        "‚îÇ           ‚îî‚îÄ‚îÄ Span: BedrockConverseLLM (LLM call)\n",
        "‚îÇ               ‚îú‚îÄ‚îÄ Input: User query\n",
        "‚îÇ               ‚îú‚îÄ‚îÄ Output: Tool call decision\n",
        "‚îÇ               ‚îî‚îÄ‚îÄ Attributes: tokens, model, temperature\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ Span: tools (tool execution)\n",
        "‚îÇ           ‚îî‚îÄ‚îÄ Span: get_stock_price (tool call)\n",
        "‚îÇ               ‚îú‚îÄ‚îÄ Input: ticker=\"AAPL\"\n",
        "‚îÇ               ‚îú‚îÄ‚îÄ Output: price data\n",
        "‚îÇ               ‚îî‚îÄ‚îÄ Attributes: ticker, price, found\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ Span: agent (final response)\n",
        "‚îÇ           ‚îî‚îÄ‚îÄ Span: BedrockConverseLLM (LLM call)\n",
        "‚îÇ               ‚îú‚îÄ‚îÄ Input: Tool result + query\n",
        "‚îÇ               ‚îî‚îÄ‚îÄ Output: Final response\n",
        "```\n",
        "\n",
        "Each span contains:\n",
        "- **Timing**: Start/end times (calculate latency)\n",
        "- **Attributes**: Key-value metadata (ticker, price, etc.)\n",
        "- **Events**: Important moments (errors, checkpoints)\n",
        "- **Context**: Links to parent/child spans\n",
        "\n",
        "**For AI/ML Applications**:\n",
        "- OpenTelemetry captures LLM calls, tool usage, and agent workflows\n",
        "- **OpenInference** extends OpenTelemetry with AI-specific conventions\n",
        "- Adds semantic meaning to spans (e.g., \"this is an LLM call\", \"this is a tool call\")\n",
        "- Captures AI-specific metadata (prompts, responses, token counts, model parameters)\n",
        "\n",
        "### Manual Tracing Basics\n",
        "\n",
        "You can create spans manually in your code:\n",
        "\n",
        "```python\n",
        "from opentelemetry import trace\n",
        "\n",
        "tracer = trace.get_tracer(__name__)\n",
        "\n",
        "# Create a span\n",
        "with tracer.start_as_current_span(\"my_operation\") as span:\n",
        "    # Add attributes (metadata)\n",
        "    span.set_attribute(\"user.id\", \"12345\")\n",
        "    span.set_attribute(\"operation.type\", \"stock_lookup\")\n",
        "    \n",
        "    # Your code here\n",
        "    result = lookup_stock(\"AAPL\")\n",
        "    \n",
        "    # Add more attributes based on results\n",
        "    span.set_attribute(\"result.price\", result[\"price\"])\n",
        "    span.set_attribute(\"result.found\", True)\n",
        "```\n",
        "\n",
        "**Key Points**:\n",
        "- Spans are context managers (use `with` statement)\n",
        "- Attributes are key-value pairs for filtering/searching\n",
        "- Spans automatically capture timing\n",
        "- Child spans are created automatically when nested\n",
        "\n",
        "## üéØ What This Demo Shows\n",
        "\n",
        "This educational demo walks you through:\n",
        "\n",
        "1. **OpenTelemetry Setup**: Configure OTLP exporter to send traces to Galileo\n",
        "   - Learn how to set up the OTLP endpoint and authentication headers\n",
        "   - Understand the relationship between OpenTelemetry and Galileo\n",
        "\n",
        "2. **OpenInference Instrumentation**: Automatic tracing of LangGraph and LangChain operations\n",
        "   - See how OpenInference adds AI-specific semantic conventions\n",
        "   - Understand automatic span creation for LLM calls and tool usage\n",
        "\n",
        "3. **AWS Bedrock Integration**: Use Bedrock models (Claude) instead of OpenAI\n",
        "   - Learn how to create a custom LangChain LLM wrapper for Bedrock\n",
        "   - Handle Bedrock ARNs and the Converse API format\n",
        "\n",
        "4. **Finance Agent Simulation**: Multi-step agentic workflow with tools\n",
        "   - Build a LangGraph agent with stock trading tools\n",
        "   - See how agentic workflows are automatically traced\n",
        "\n",
        "5. **Complete Trace Visibility**: View full workflow traces in Galileo Console\n",
        "   - See complete trace graphs showing the entire agent workflow\n",
        "   - Understand how spans connect to show the full execution path\n",
        "\n",
        "### Why OpenTelemetry + Galileo?\n",
        "\n",
        "**OpenTelemetry** provides the instrumentation and data collection, while **Galileo** provides:\n",
        "- **AI-Specific Visualization**: Trace graphs optimized for LLM workflows\n",
        "- **Token Usage Tracking**: Automatic tracking of input/output tokens\n",
        "- **Cost Analysis**: Calculate costs per request based on model pricing\n",
        "- **Performance Monitoring**: Latency and throughput metrics for AI operations\n",
        "- **Debugging**: See exactly what your agent did and why\n",
        "\n",
        "**Together**: OpenTelemetry collects the data, Galileo makes it actionable for AI/ML applications.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "1. **Python 3.9+** installed\n",
        "2. **Galileo Account**: Free account at [app.galileo.ai](https://app.galileo.ai)\n",
        "3. **AWS Account** with Bedrock access\n",
        "4. **Secrets Configuration**: Create `.streamlit/secrets.toml` with:\n",
        "   - `galileo_api_key` - Your Galileo API key\n",
        "   - `galileo_project` - Your project name\n",
        "   - `galileo_log_stream` - Your log stream name\n",
        "   - `aws_access_key_id` - AWS access key (optional, can use env vars)\n",
        "   - `aws_secret_access_key` - AWS secret key (optional, can use env vars)\n",
        "   - `aws_default_region` - AWS region (optional, defaults to us-east-1)\n",
        "   \n",
        "   **Note**: AWS credentials can also be set via environment variables or AWS credentials file.\n",
        "\n",
        "## üìö Documentation\n",
        "\n",
        "This notebook follows the [Galileo OpenTelemetry Integration Guide](https://v2docs.galileo.ai/how-to-guides/third-party-integrations/otel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  .streamlit/secrets.toml not found. Please create it with your API keys.\n",
            "‚úÖ Environment configured (using same setup as Streamlit app)\n",
            "‚úÖ Configuration loaded successfully from secrets.toml!\n",
            "   üìä Project: cwan_demo\n",
            "   üìù Log Stream: cwan_demo_logs\n",
            "   üåç AWS Region: us-east-1\n",
            "   ü§ñ Bedrock Model: arn:aws:bedrock:us-east-1:818240400754:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0\n",
            "   üìÅ Project Root: /Users/sabinaashurova/Desktop/GitHub/galileo-golden-demo\n",
            "   üîê Secrets loaded from: /Users/sabinaashurova/Desktop/GitHub/galileo-golden-demo/.streamlit/secrets.toml\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup Environment and Configuration\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import toml\n",
        "\n",
        "# Add project root to path (for importing project modules)\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == \"OpenTelemetry_notebooks\":\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Use the same setup_environment function as the Streamlit app\n",
        "from setup_env import setup_environment\n",
        "\n",
        "# Load secrets and set environment variables (same as Streamlit app)\n",
        "setup_environment()\n",
        "\n",
        "# Load secrets from .streamlit/secrets.toml for direct access in notebook\n",
        "secrets_path = project_root / \".streamlit\" / \"secrets.toml\"\n",
        "\n",
        "if not secrets_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"‚ùå Secrets file not found: {secrets_path}\\n\"\n",
        "        f\"   Please create it from the template: .streamlit/secrets.toml.template\"\n",
        "    )\n",
        "\n",
        "try:\n",
        "    secrets = toml.load(secrets_path)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"‚ùå Error loading secrets.toml: {e}\")\n",
        "\n",
        "# Configuration - Load from secrets.toml (for notebook use)\n",
        "GALILEO_API_KEY = secrets.get(\"galileo_api_key\", \"\") or os.getenv(\"GALILEO_API_KEY\", \"\")\n",
        "GALILEO_PROJECT = secrets.get(\"galileo_project\", \"otel-demo\") or os.getenv(\"GALILEO_PROJECT\", \"otel-demo\")\n",
        "GALILEO_LOG_STREAM = secrets.get(\"galileo_log_stream\", \"finance-agent\") or os.getenv(\"GALILEO_LOG_STREAM\", \"finance-agent\")\n",
        "\n",
        "# AWS Configuration - Check secrets.toml first, then environment variables\n",
        "AWS_ACCESS_KEY_ID = secrets.get(\"aws_access_key_id\", \"\") or os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\n",
        "AWS_SECRET_ACCESS_KEY = secrets.get(\"aws_secret_access_key\", \"\") or os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
        "AWS_DEFAULT_REGION = secrets.get(\"aws_default_region\", \"\") or os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
        "\n",
        "# Bedrock Model ID or ARN - from secrets or env\n",
        "BEDROCK_MODEL_ID = (\n",
        "    secrets.get(\"bedrock_model_arn\", \"\") or \n",
        "    secrets.get(\"bedrock_model_id\", \"\") or \n",
        "    os.getenv(\"BEDROCK_MODEL_ARN\", \"\") or\n",
        "    os.getenv(\"BEDROCK_MODEL_ID\", \"anthropic.claude-3-5-sonnet-20241022-v2:0\")\n",
        ")\n",
        "\n",
        "# Validate required configuration\n",
        "if not GALILEO_API_KEY:\n",
        "    raise ValueError(\"‚ùå galileo_api_key must be set in secrets.toml. Get it from: https://app.galileo.ai/settings/api-keys\")\n",
        "\n",
        "if not GALILEO_PROJECT:\n",
        "    raise ValueError(\"‚ùå galileo_project must be set in secrets.toml\")\n",
        "\n",
        "if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:\n",
        "    raise ValueError(\n",
        "        \"‚ùå AWS credentials must be set. Add to secrets.toml:\\n\"\n",
        "        \"   aws_access_key_id = \\\"your-key\\\"\\n\"\n",
        "        \"   aws_secret_access_key = \\\"your-secret\\\"\\n\"\n",
        "        \"   Or set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables\"\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Environment configured (using same setup as Streamlit app)\")\n",
        "\n",
        "print(\"‚úÖ Configuration loaded successfully from secrets.toml!\")\n",
        "print(f\"   üìä Project: {GALILEO_PROJECT}\")\n",
        "print(f\"   üìù Log Stream: {GALILEO_LOG_STREAM}\")\n",
        "print(f\"   üåç AWS Region: {AWS_DEFAULT_REGION}\")\n",
        "print(f\"   ü§ñ Bedrock Model: {BEDROCK_MODEL_ID}\")\n",
        "print(f\"   üìÅ Project Root: {project_root}\")\n",
        "print(f\"   üîê Secrets loaded from: {secrets_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure OpenTelemetry with Galileo\n",
        "\n",
        "Following the [Galileo OpenTelemetry docs](https://v2docs.galileo.ai/how-to-guides/third-party-integrations/otel), we'll:\n",
        "1. Set up OTLP endpoint to send traces to Galileo\n",
        "2. Configure headers in the required format\n",
        "3. Create tracer provider with resource information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó OTLP Endpoint: https://api.galileo.ai/otel/traces\n",
            "   ‚úÖ This is the correct endpoint (confirmed by testing)\n",
            "‚úÖ OpenTelemetry configuration:\n",
            "   üîó Endpoint: https://api.galileo.ai/otel/traces\n",
            "   üîë Headers configured: Galileo-API-Key, project, logstream\n",
            "   üìã Header format: Galileo-API-Key=6GXiils0hsvDRogLRcKv_beYim-2pWwxe6...\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Configure OpenTelemetry OTLP Endpoint and Headers\n",
        "# Following Galileo docs: https://v2docs.galileo.ai/how-to-guides/third-party-integrations/otel\n",
        "\n",
        "# Galileo's OpenTelemetry endpoint\n",
        "# Per Galileo docs: https://v2docs.galileo.ai/how-to-guides/third-party-integrations/otel\n",
        "# The endpoint is: https://api.galileo.ai/otel/traces\n",
        "#\n",
        "# IMPORTANT: Testing confirmed that https://api.galileo.ai/otel/traces returns 422\n",
        "# (which means endpoint is correct and authentication works!)\n",
        "#\n",
        "# The OTLP HTTP exporter may append /v1/traces, but we need to use the exact endpoint\n",
        "# Let's pass the full endpoint and see if the exporter is smart enough to not append\n",
        "GALILEO_OTLP_ENDPOINT = \"https://api.galileo.ai/otel/traces\"\n",
        "print(f\"üîó OTLP Endpoint: {GALILEO_OTLP_ENDPOINT}\")\n",
        "print(f\"   ‚úÖ This is the correct endpoint (confirmed by testing)\")\n",
        "\n",
        "# Configure headers in the format required by OpenTelemetry\n",
        "# IMPORTANT: OTel requires comma-separated string format, not a dictionary!\n",
        "headers = {\n",
        "    \"Galileo-API-Key\": GALILEO_API_KEY,\n",
        "    \"project\": GALILEO_PROJECT,\n",
        "    \"logstream\": GALILEO_LOG_STREAM,\n",
        "}\n",
        "\n",
        "# Set environment variable in the format OpenTelemetry expects\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"] = \",\".join(\n",
        "    [f\"{k}={v}\" for k, v in headers.items()]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ OpenTelemetry configuration:\")\n",
        "print(f\"   üîó Endpoint: {GALILEO_OTLP_ENDPOINT}\")\n",
        "print(f\"   üîë Headers configured: {', '.join(headers.keys())}\")\n",
        "print(f\"   üìã Header format: {os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS'][:50]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Initialize OpenTelemetry\n",
        "\n",
        "**What we're doing**: Creating the OpenTelemetry tracer provider and configuring exporters.\n",
        "\n",
        "### Understanding the Tracer Provider\n",
        "\n",
        "The **Tracer Provider** is the core component of OpenTelemetry:\n",
        "\n",
        "1. **Creates Tracers**: Tracers are used to create spans in your code\n",
        "2. **Manages Spans**: Collects all spans created by your application\n",
        "3. **Processes Spans**: Batches spans for efficient delivery\n",
        "4. **Exports Spans**: Sends spans to exporters (Galileo in our case)\n",
        "\n",
        "**Components**:\n",
        "- **TracerProvider**: The main manager\n",
        "- **Span Processor**: Batches and processes spans before export\n",
        "- **Exporter**: Sends spans to Galileo via OTLP\n",
        "- **Resource**: Metadata about your service (name, version, etc.)\n",
        "\n",
        "We use `setup_opentelemetry_from_env()` which:\n",
        "- Creates a `TracerProvider` that manages trace creation\n",
        "- Configures the OTLP exporter to send traces to Galileo\n",
        "- Sets up batch processing for efficient trace delivery\n",
        "- Enables automatic instrumentation for common libraries\n",
        "\n",
        "**Why this matters**: \n",
        "- The tracer provider is the \"engine\" that powers OpenTelemetry\n",
        "- It receives spans from your code (manual or automatic)\n",
        "- It batches them efficiently (not every span immediately)\n",
        "- It sends them to Galileo when ready\n",
        "- Using the same setup function as the Streamlit app ensures consistency and reliability\n",
        "\n",
        "### How Spans Flow\n",
        "\n",
        "```\n",
        "Your Code ‚Üí Creates Span ‚Üí Tracer Provider ‚Üí Batch Processor ‚Üí Exporter ‚Üí Galileo\n",
        "                                                      ‚Üì\n",
        "                                                (Batched for efficiency)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI instrumentation enabled\n",
            "‚úÖ HTTP client instrumentation enabled\n",
            "‚úÖ OpenInference instrumentations enabled (OpenAI + LangChain)\n",
            "üîß OpenTelemetry setup complete!\n",
            "‚úÖ OpenTelemetry initialized (using same setup as Streamlit app)\n",
            "   üìä Service: finance-agent-demo\n",
            "   üì¶ Traces will be sent to Galileo\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Initialize OpenTelemetry\n",
        "# Use the same setup function as the Streamlit app for consistency\n",
        "from setup_otel import setup_opentelemetry_from_env\n",
        "\n",
        "# setup_environment() already set the environment variables\n",
        "# Now use setup_opentelemetry_from_env() which reads from env vars (same as Streamlit app)\n",
        "try:\n",
        "    tracer_provider = setup_opentelemetry_from_env()\n",
        "    print(\"‚úÖ OpenTelemetry initialized (using same setup as Streamlit app)\")\n",
        "    print(f\"   üìä Service: {os.getenv('OTEL_SERVICE_NAME', 'finance-agent-demo')}\")\n",
        "    print(f\"   üì¶ Traces will be sent to Galileo\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up OpenTelemetry: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Apply OpenInference Instrumentation\n",
        "\n",
        "**What we're doing**: Enabling automatic tracing for AI/ML frameworks.\n",
        "\n",
        "### What is OpenInference?\n",
        "\n",
        "**OpenInference** is an extension of OpenTelemetry that adds AI/ML-specific semantic conventions:\n",
        "- **Standard Attributes**: Consistent naming for LLM calls, tool usage, etc.\n",
        "- **Automatic Instrumentation**: Wraps common AI frameworks to create spans automatically\n",
        "- **Rich Metadata**: Captures AI-specific information (prompts, tokens, model params)\n",
        "\n",
        "### How Automatic Instrumentation Works\n",
        "\n",
        "When you call `LangChainInstrumentor().instrument()`, it:\n",
        "1. **Wraps LangChain code**: Intercepts LangChain operations\n",
        "2. **Creates spans automatically**: Every chain, tool call, and LLM invocation gets a span\n",
        "3. **Adds metadata**: Extracts input/output, parameters, token counts, etc.\n",
        "4. **Maintains context**: Spans are linked to show the full workflow\n",
        "\n",
        "**What gets instrumented**:\n",
        "- **LangGraph** workflows and nodes - captures the agent's decision-making flow\n",
        "- **LangChain** chains and tools - tracks tool calls and LLM interactions  \n",
        "- **LLM API calls** - captures prompts, responses, token usage, and model parameters\n",
        "\n",
        "**Why this matters**: \n",
        "Without OpenInference, you'd need to manually create spans for every LLM call and tool invocation:\n",
        "```python\n",
        "# Without OpenInference (manual):\n",
        "with tracer.start_as_current_span(\"llm_call\") as span:\n",
        "    span.set_attribute(\"input\", prompt)\n",
        "    response = llm.invoke(prompt)\n",
        "    span.set_attribute(\"output\", response)\n",
        "    span.set_attribute(\"tokens\", tokens_used)\n",
        "\n",
        "# With OpenInference (automatic):\n",
        "response = llm.invoke(prompt)  # Span created automatically!\n",
        "```\n",
        "\n",
        "OpenInference adds rich metadata automatically:\n",
        "- Input/output messages\n",
        "- Token counts\n",
        "- Model parameters (temperature, max_tokens, etc.)\n",
        "- Tool names and arguments\n",
        "- Workflow step information\n",
        "\n",
        "This gives you complete visibility into your agent's behavior without writing trace code for every operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **LangChain** chains and tools\n",
        "- **OpenAI** API calls (we'll use Bedrock instead)\n",
        "\n",
        "This enables automatic tracing of LLM calls, token usage, and agent workflow steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LangChain instrumentation enabled\n",
            "‚ö†Ô∏è  LangGraph instrumentation not available\n",
            "   Note: openinference-instrumentation-langgraph may not exist as a package\n",
            "   Manual tracing will be used instead\n",
            "\n",
            "‚úÖ OpenInference instrumentation enabled:\n",
            "   üîó LangChain operations will be traced\n",
            "   üìä LLM calls, token usage, and agent steps will be captured automatically\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Apply OpenInference Instrumentation\n",
        "# This automatically captures LangGraph workflows, LangChain operations, and LLM calls\n",
        "\n",
        "langchain_instrumented = False\n",
        "langgraph_instrumented = False\n",
        "\n",
        "try:\n",
        "    from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "    \n",
        "    # Instrument LangChain\n",
        "    LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "    langchain_instrumented = True\n",
        "    print(\"‚úÖ LangChain instrumentation enabled\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è  LangChain instrumentation not available: {e}\")\n",
        "    print(\"   Install with: pip install openinference-instrumentation-langchain\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error enabling LangChain instrumentation: {e}\")\n",
        "\n",
        "try:\n",
        "    from openinference.instrumentation.langgraph import LangGraphInstrumentor\n",
        "    \n",
        "    # Instrument LangGraph\n",
        "    LangGraphInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "    langgraph_instrumented = True\n",
        "    print(\"‚úÖ LangGraph instrumentation enabled\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  LangGraph instrumentation not available\")\n",
        "    print(\"   Note: openinference-instrumentation-langgraph may not exist as a package\")\n",
        "    print(\"   Manual tracing will be used instead\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error enabling LangGraph instrumentation: {e}\")\n",
        "\n",
        "if langchain_instrumented or langgraph_instrumented:\n",
        "    print(\"\\n‚úÖ OpenInference instrumentation enabled:\")\n",
        "    if langchain_instrumented:\n",
        "        print(\"   üîó LangChain operations will be traced\")\n",
        "    if langgraph_instrumented:\n",
        "        print(\"   üîó LangGraph workflows will be traced\")\n",
        "    print(\"   üìä LLM calls, token usage, and agent steps will be captured automatically\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No OpenInference instrumentation available\")\n",
        "    print(\"   Manual spans will still work, but automatic tracing is disabled\")\n",
        "    print(\"   Traces should still appear in Galileo from manual spans\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Finance Agent Tools\n",
        "\n",
        "**What we're doing**: Creating LangChain tools that the agent can use to perform actions.\n",
        "\n",
        "### Understanding Manual vs Automatic Tracing\n",
        "\n",
        "**Automatic Tracing** (OpenInference):\n",
        "- Created by instrumentation libraries\n",
        "- Happens automatically when you call LangChain/LangGraph functions\n",
        "- Captures standard information (inputs, outputs, timing)\n",
        "\n",
        "**Manual Tracing** (Custom spans):\n",
        "- Created explicitly in your code with `tracer.start_as_current_span()`\n",
        "- Lets you add custom attributes and context\n",
        "- Useful for business logic or custom operations\n",
        "\n",
        "**In this demo**: We use both!\n",
        "- OpenInference automatically traces tool calls\n",
        "- We also add manual spans to include custom attributes (e.g., stock ticker, order ID)\n",
        "\n",
        "### The Tools\n",
        "\n",
        "We'll create three finance tools:\n",
        "- `get_stock_price`: Look up current stock prices and market data\n",
        "- `purchase_stocks`: Execute a stock purchase order\n",
        "- `sell_stocks`: Execute a stock sale order\n",
        "\n",
        "**Why this matters**: These tools demonstrate how agents can:\n",
        "1. **Use external data**: `get_stock_price` accesses market data\n",
        "2. **Perform actions**: `purchase_stocks` and `sell_stocks` execute transactions\n",
        "3. **Be automatically traced**: Each tool call creates a span with input/output data\n",
        "\n",
        "**Note**: Each tool includes manual OpenTelemetry spans (`tracer.start_as_current_span`) to add custom attributes like `ticker`, `order.id`, etc. OpenInference will also automatically create spans for tool calls, so you'll see both in Galileo - giving you comprehensive visibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Finance tools created:\n",
            "   üîß get_stock_price: Get the current stock price and market data for a given ticker symbol (e.g., AAPL, MSFT, TSLA).\n",
            "   üîß purchase_stocks: Execute a stock purchase order. Returns order confirmation with order ID.\n",
            "   üîß sell_stocks: Execute a stock sale order. Returns order confirmation with order ID.\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Create Finance Agent Tools\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from typing import Optional\n",
        "from langchain_core.tools import tool\n",
        "from opentelemetry import trace\n",
        "\n",
        "tracer = trace.get_tracer(__name__)\n",
        "\n",
        "# Mock stock price database\n",
        "MOCK_STOCKS = {\n",
        "    \"AAPL\": {\"price\": 178.72, \"change\": 1.23, \"change_percent\": 0.69},\n",
        "    \"MSFT\": {\"price\": 415.32, \"change\": 2.45, \"change_percent\": 0.59},\n",
        "    \"GOOGL\": {\"price\": 147.68, \"change\": -0.82, \"change_percent\": -0.55},\n",
        "    \"AMZN\": {\"price\": 178.75, \"change\": 1.25, \"change_percent\": 0.70},\n",
        "    \"TSLA\": {\"price\": 177.77, \"change\": -2.33, \"change_percent\": -1.29},\n",
        "    \"NVDA\": {\"price\": 950.02, \"change\": 15.98, \"change_percent\": 1.71},\n",
        "}\n",
        "\n",
        "@tool\n",
        "def get_stock_price(ticker: str) -> str:\n",
        "    \"\"\"Get the current stock price and market data for a given ticker symbol (e.g., AAPL, MSFT, TSLA).\"\"\"\n",
        "    with tracer.start_as_current_span(\"get_stock_price\") as span:\n",
        "        span.set_attribute(\"ticker\", ticker)\n",
        "        \n",
        "        if ticker.upper() in MOCK_STOCKS:\n",
        "            data = MOCK_STOCKS[ticker.upper()]\n",
        "            span.set_attribute(\"stock.found\", True)\n",
        "            span.set_attribute(\"stock.price\", data[\"price\"])\n",
        "            return json.dumps(data)\n",
        "        else:\n",
        "            # Default mock data\n",
        "            span.set_attribute(\"stock.found\", False)\n",
        "            result = {\"price\": 100.00, \"change\": 0.00, \"change_percent\": 0.00}\n",
        "            return json.dumps(result)\n",
        "\n",
        "@tool\n",
        "def purchase_stocks(ticker: str, quantity: int, price: float) -> str:\n",
        "    \"\"\"Execute a stock purchase order. Returns order confirmation with order ID.\"\"\"\n",
        "    with tracer.start_as_current_span(\"purchase_stocks\") as span:\n",
        "        span.set_attribute(\"ticker\", ticker)\n",
        "        span.set_attribute(\"quantity\", quantity)\n",
        "        span.set_attribute(\"price\", price)\n",
        "        \n",
        "        order_id = f\"ORD-{random.randint(100000, 999999)}\"\n",
        "        total_cost = quantity * price\n",
        "        fees = 10.00\n",
        "        \n",
        "        result = {\n",
        "            \"order_id\": order_id,\n",
        "            \"ticker\": ticker,\n",
        "            \"quantity\": quantity,\n",
        "            \"price\": price,\n",
        "            \"total_cost\": total_cost,\n",
        "            \"fees\": fees,\n",
        "            \"total_with_fees\": total_cost + fees,\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        \n",
        "        span.set_attribute(\"order.id\", order_id)\n",
        "        span.set_attribute(\"order.total\", total_cost + fees)\n",
        "        \n",
        "        return json.dumps(result)\n",
        "\n",
        "@tool\n",
        "def sell_stocks(ticker: str, quantity: int, price: float) -> str:\n",
        "    \"\"\"Execute a stock sale order. Returns order confirmation with order ID.\"\"\"\n",
        "    with tracer.start_as_current_span(\"sell_stocks\") as span:\n",
        "        span.set_attribute(\"ticker\", ticker)\n",
        "        span.set_attribute(\"quantity\", quantity)\n",
        "        span.set_attribute(\"price\", price)\n",
        "        \n",
        "        order_id = f\"ORD-{random.randint(100000, 999999)}\"\n",
        "        total_sale = quantity * price\n",
        "        fees = 14.99\n",
        "        \n",
        "        result = {\n",
        "            \"order_id\": order_id,\n",
        "            \"ticker\": ticker,\n",
        "            \"quantity\": quantity,\n",
        "            \"price\": price,\n",
        "            \"total_sale\": total_sale,\n",
        "            \"fees\": fees,\n",
        "            \"total_with_fees\": total_sale - fees,\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        \n",
        "        span.set_attribute(\"order.id\", order_id)\n",
        "        span.set_attribute(\"order.total\", total_sale - fees)\n",
        "        \n",
        "        return json.dumps(result)\n",
        "\n",
        "# Create tools list\n",
        "finance_tools = [get_stock_price, purchase_stocks, sell_stocks]\n",
        "\n",
        "print(\"‚úÖ Finance tools created:\")\n",
        "for tool in finance_tools:\n",
        "    print(f\"   üîß {tool.name}: {tool.description}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Bedrock runtime client initialized\n"
          ]
        }
      ],
      "source": [
        "# Step 6a: Initialize Bedrock Client\n",
        "import boto3\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.outputs import ChatGeneration, ChatResult\n",
        "from typing import List, Optional\n",
        "\n",
        "# Initialize AWS Bedrock client (same as your working code)\n",
        "bedrock_runtime = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    region_name=AWS_DEFAULT_REGION\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Bedrock runtime client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6b: Create Custom Bedrock LLM Wrapper\n",
        "\n",
        "**What we're doing**: Creating a custom LangChain LLM wrapper that uses AWS Bedrock's Converse API.\n",
        "\n",
        "**Why we need this**: \n",
        "- LangChain's `ChatBedrock` requires a `model_provider` parameter when using ARNs\n",
        "- Bedrock's Converse API works directly with ARNs without provider info\n",
        "- This wrapper gives us full control over the API calls and message formatting\n",
        "\n",
        "**How it works**:\n",
        "1. Converts LangChain messages to Bedrock's message format\n",
        "2. Handles tool calls and tool results (required for agent workflows)\n",
        "3. Converts Bedrock responses back to LangChain format\n",
        "4. Maintains trace context for OpenTelemetry\n",
        "\n",
        "**Key features**:\n",
        "- Works directly with Bedrock ARNs (e.g., `arn:aws:bedrock:us-east-1:...`)\n",
        "- Handles tool calling (required for agentic workflows)\n",
        "- Properly formats tool results to match Bedrock's expected format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Custom BedrockConverseLLM class defined\n"
          ]
        }
      ],
      "source": [
        "# Step 6b: Create Custom Bedrock LLM Wrapper\n",
        "# This uses boto3.converse directly (same as your working code)\n",
        "from pydantic import Field, ConfigDict\n",
        "from typing import Any\n",
        "\n",
        "class BedrockConverseLLM(BaseChatModel):\n",
        "    \"\"\"Custom LangChain LLM that uses boto3 Bedrock Converse API (works with ARNs)\"\"\"\n",
        "    \n",
        "    model_id: str = Field(description=\"Bedrock model ID or ARN\")\n",
        "    bedrock_client: Any = Field(description=\"boto3 bedrock-runtime client\")\n",
        "    temperature: float = Field(default=0.1, description=\"Temperature for generation\")\n",
        "    max_tokens: int = Field(default=2000, description=\"Max tokens for generation\")\n",
        "    bound_tools: Optional[List[Any]] = Field(default=None, description=\"Bound tools for the LLM\")\n",
        "    \n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "    \n",
        "    def __init__(self, model_id: str, bedrock_client, **kwargs):\n",
        "        super().__init__(\n",
        "            model_id=model_id,\n",
        "            bedrock_client=bedrock_client,\n",
        "            temperature=kwargs.get(\"temperature\", 0.1),\n",
        "            max_tokens=kwargs.get(\"max_tokens\", 2000),\n",
        "            bound_tools=kwargs.get(\"tools\", None),\n",
        "            **{k: v for k, v in kwargs.items() if k not in [\"temperature\", \"max_tokens\", \"tools\"]}\n",
        "        )\n",
        "    \n",
        "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, **kwargs):\n",
        "        # Convert LangChain messages to Bedrock format\n",
        "        # Important: Bedrock Converse doesn't allow assistant messages in the final position when using tools\n",
        "        bedrock_messages = []\n",
        "        # Track tool use IDs from tool calls to match with tool results\n",
        "        tool_use_id_map = {}  # Maps LangChain tool_call_id to Bedrock toolUseId\n",
        "        \n",
        "        for i, msg in enumerate(messages):\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                bedrock_messages.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [{\"text\": msg.content}]\n",
        "                })\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                # Handle tool calls if present\n",
        "                if hasattr(msg, \"tool_calls\") and msg.tool_calls and len(msg.tool_calls) > 0:\n",
        "                    # Convert tool calls to Bedrock format\n",
        "                    content = []\n",
        "                    for tool_call in msg.tool_calls:\n",
        "                        # Generate a unique toolUseId for Bedrock\n",
        "                        langchain_tool_call_id = tool_call.get(\"id\", \"\")\n",
        "                        # Use the LangChain tool_call_id as the Bedrock toolUseId\n",
        "                        # Bedrock expects toolUseId to match in tool calls and tool results\n",
        "                        bedrock_tool_use_id = langchain_tool_call_id if langchain_tool_call_id else f\"tooluse_{i}_{len(content)}\"\n",
        "                        \n",
        "                        # Map LangChain tool_call_id to Bedrock toolUseId\n",
        "                        if langchain_tool_call_id:\n",
        "                            tool_use_id_map[langchain_tool_call_id] = bedrock_tool_use_id\n",
        "                        \n",
        "                        content.append({\n",
        "                            \"toolUse\": {\n",
        "                                \"toolUseId\": bedrock_tool_use_id,\n",
        "                                \"name\": tool_call.get(\"name\", \"\"),\n",
        "                                \"input\": tool_call.get(\"args\", {})\n",
        "                            }\n",
        "                        })\n",
        "                    bedrock_messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": content\n",
        "                    })\n",
        "                elif msg.content and msg.content.strip():\n",
        "                    # Only add assistant text message if it's not the last message\n",
        "                    # (Bedrock doesn't allow assistant message as final message when tools are enabled)\n",
        "                    is_last_message = (i == len(messages) - 1)\n",
        "                    if not is_last_message:\n",
        "                        bedrock_messages.append({\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"content\": [{\"text\": msg.content}]\n",
        "                        })\n",
        "            elif isinstance(msg, SystemMessage):\n",
        "                # Bedrock Converse uses system message in a different way\n",
        "                bedrock_messages.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [{\"text\": msg.content}]\n",
        "                })\n",
        "            # Handle ToolMessage if present (tool execution results)\n",
        "            elif isinstance(msg, ToolMessage):\n",
        "                # ToolMessage from LangChain - convert to Bedrock tool result format\n",
        "                # ToolMessage has tool_call_id attribute that matches the LangChain tool_call id\n",
        "                langchain_tool_call_id = getattr(msg, \"tool_call_id\", None)\n",
        "                tool_content = str(msg.content)\n",
        "                \n",
        "                # Map LangChain tool_call_id to Bedrock toolUseId\n",
        "                bedrock_tool_use_id = tool_use_id_map.get(langchain_tool_call_id) if langchain_tool_call_id else None\n",
        "                \n",
        "                if not bedrock_tool_use_id:\n",
        "                    # Fallback: try to use the tool_call_id directly if it looks like a Bedrock toolUseId\n",
        "                    if langchain_tool_call_id and langchain_tool_call_id.startswith(\"tooluse_\"):\n",
        "                        bedrock_tool_use_id = langchain_tool_call_id\n",
        "                    else:\n",
        "                        # Skip if we can't find the matching tool use ID\n",
        "                        print(f\"‚ö†Ô∏è  Warning: Could not map tool_call_id {langchain_tool_call_id} to Bedrock toolUseId, skipping tool result\")\n",
        "                        continue\n",
        "                \n",
        "                tool_result = {\n",
        "                    \"toolResult\": {\n",
        "                        \"toolUseId\": bedrock_tool_use_id,\n",
        "                        \"status\": \"success\",\n",
        "                        \"content\": [{\"text\": tool_content}]\n",
        "                    }\n",
        "                }\n",
        "                bedrock_messages.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [tool_result]\n",
        "                })\n",
        "        \n",
        "        # Prepare tool config if tools are bound\n",
        "        tool_config = None\n",
        "        if self.bound_tools:\n",
        "            # Convert LangChain tools to Bedrock format\n",
        "            bedrock_tools = []\n",
        "            for tool in self.bound_tools:\n",
        "                tool_spec = {\n",
        "                    \"toolSpec\": {\n",
        "                        \"name\": tool.name,\n",
        "                        \"description\": tool.description or \"\",\n",
        "                        \"inputSchema\": {}\n",
        "                    }\n",
        "                }\n",
        "                # Try to get input schema from tool\n",
        "                # Bedrock expects inputSchema as {\"json\": <schema>} format\n",
        "                if hasattr(tool, \"args_schema\") and tool.args_schema:\n",
        "                    try:\n",
        "                        # Use model_json_schema for Pydantic v2\n",
        "                        if hasattr(tool.args_schema, \"model_json_schema\"):\n",
        "                            schema = tool.args_schema.model_json_schema()\n",
        "                        elif hasattr(tool.args_schema, \"schema\"):\n",
        "                            schema = tool.args_schema.schema()\n",
        "                        else:\n",
        "                            schema = {}\n",
        "                        # Bedrock requires inputSchema in {\"json\": <schema>} format\n",
        "                        tool_spec[\"toolSpec\"][\"inputSchema\"] = {\"json\": schema}\n",
        "                    except Exception:\n",
        "                        # Fallback to empty schema\n",
        "                        tool_spec[\"toolSpec\"][\"inputSchema\"] = {\"json\": {}}\n",
        "                else:\n",
        "                    # No schema available, use empty JSON schema\n",
        "                    tool_spec[\"toolSpec\"][\"inputSchema\"] = {\"json\": {}}\n",
        "                \n",
        "                bedrock_tools.append(tool_spec)\n",
        "            \n",
        "            if bedrock_tools:\n",
        "                tool_config = {\"tools\": bedrock_tools}\n",
        "        \n",
        "        # Call Bedrock Converse API (works with ARNs directly)\n",
        "        converse_kwargs = {\n",
        "            \"modelId\": self.model_id,  # Your ARN works here\n",
        "            \"messages\": bedrock_messages,\n",
        "            \"inferenceConfig\": {\n",
        "                \"maxTokens\": self.max_tokens,\n",
        "                \"temperature\": self.temperature\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        if tool_config:\n",
        "            converse_kwargs[\"toolConfig\"] = tool_config\n",
        "        \n",
        "        response = self.bedrock_client.converse(**converse_kwargs)\n",
        "        \n",
        "        # Extract response - handle both text and tool use\n",
        "        output = response['output']['message']['content'][0]\n",
        "        \n",
        "        if 'text' in output:\n",
        "            # Regular text response\n",
        "            output_text = output['text']\n",
        "            message = AIMessage(content=output_text)\n",
        "        elif 'toolUse' in output:\n",
        "            # Tool use response - convert to LangChain format\n",
        "            tool_use = output['toolUse']\n",
        "            message = AIMessage(\n",
        "                content=\"\",\n",
        "                tool_calls=[{\n",
        "                    \"id\": tool_use.get(\"toolUseId\", \"\"),\n",
        "                    \"name\": tool_use.get(\"name\", \"\"),\n",
        "                    \"args\": tool_use.get(\"input\", {})\n",
        "                }]\n",
        "            )\n",
        "        else:\n",
        "            output_text = str(output)\n",
        "            message = AIMessage(content=output_text)\n",
        "        \n",
        "        # Return ChatResult\n",
        "        generation = ChatGeneration(message=message)\n",
        "        return ChatResult(generations=[generation])\n",
        "    \n",
        "    def bind_tools(self, tools, **kwargs):\n",
        "        \"\"\"Bind tools to the LLM\"\"\"\n",
        "        return BedrockConverseLLM(\n",
        "            model_id=self.model_id,\n",
        "            bedrock_client=self.bedrock_client,\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "            tools=tools\n",
        "        )\n",
        "    \n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"bedrock-converse\"\n",
        "\n",
        "print(\"‚úÖ Custom BedrockConverseLLM class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6c: Initialize LLM and Bind Tools\n",
        "\n",
        "**What we're doing**: Creating the LLM instance and attaching tools to it.\n",
        "\n",
        "1. **Initialize the LLM**: Create our custom `BedrockConverseLLM` with the model ARN\n",
        "2. **Bind tools**: Attach the finance tools to the LLM so it can use them\n",
        "\n",
        "**Why this matters**: When tools are bound to an LLM, the LLM can decide when to call them based on the user's request. The agent will automatically:\n",
        "- Analyze the user's query\n",
        "- Decide which tools to use\n",
        "- Call the tools with appropriate parameters\n",
        "- Use the tool results to generate a response\n",
        "\n",
        "This binding is what enables the agentic behavior - the LLM becomes an agent that can take actions, not just respond with text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Bedrock LLM initialized (ARN): arn:aws:bedrock:us-east-1:818240400754:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0\n",
            "   üîß Tools bound: 3\n",
            "   üí° Using boto3.converse directly (same as your working code)\n"
          ]
        }
      ],
      "source": [
        "# Step 6c: Initialize LLM and Bind Tools\n",
        "try:\n",
        "    is_arn = BEDROCK_MODEL_ID.startswith(\"arn:\")\n",
        "    \n",
        "    llm = BedrockConverseLLM(\n",
        "        model_id=BEDROCK_MODEL_ID,  # Your ARN works directly here\n",
        "        bedrock_client=bedrock_runtime,\n",
        "        temperature=0.1,\n",
        "        max_tokens=2000\n",
        "    )\n",
        "    \n",
        "    # Bind tools to the LLM\n",
        "    llm_with_tools = llm.bind_tools(finance_tools)\n",
        "    \n",
        "    model_type = \"ARN\" if is_arn else \"Model ID\"\n",
        "    print(f\"‚úÖ Bedrock LLM initialized ({model_type}): {BEDROCK_MODEL_ID}\")\n",
        "    print(f\"   üîß Tools bound: {len(finance_tools)}\")\n",
        "    print(f\"   üí° Using boto3.converse directly (same as your working code)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing Bedrock: {e}\")\n",
        "    print(\"   Make sure your AWS credentials are correct and Bedrock access is enabled\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6d: Build LangGraph Workflow\n",
        "\n",
        "**What we're doing**: Creating the agent workflow using LangGraph.\n",
        "\n",
        "**LangGraph structure**:\n",
        "1. **Agent node**: The LLM that decides what to do next\n",
        "2. **Tools node**: Executes tool calls when the agent decides to use tools\n",
        "3. **Conditional routing**: Determines whether to continue to tools or end\n",
        "\n",
        "**How it works**:\n",
        "```\n",
        "User Query ‚Üí Agent ‚Üí [Decision: Continue to Tools OR End]\n",
        "                      ‚Üì\n",
        "                   Tools Node ‚Üí Agent ‚Üí [Decision: Continue OR End]\n",
        "```\n",
        "\n",
        "**Why LangGraph**: \n",
        "- Provides a structured way to build agentic workflows\n",
        "- Handles tool calling automatically\n",
        "- Manages state and conversation context\n",
        "- Automatically traced by OpenInference (if available)\n",
        "\n",
        "**What you'll see in traces**: Each node execution creates a span, showing the complete flow of the agent's decision-making process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LangGraph agent created with workflow:\n",
            "   START ‚Üí agent ‚Üí (tools if needed) ‚Üí agent ‚Üí END\n"
          ]
        }
      ],
      "source": [
        "# Step 6d: Build LangGraph Workflow\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from typing import Annotated, TypedDict\n",
        "\n",
        "# Define state for the graph\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Create the graph\n",
        "def create_agent_node(llm):\n",
        "    \"\"\"Create the agent node that processes messages\"\"\"\n",
        "    def agent_node(state: AgentState):\n",
        "        messages = [SystemMessage(content=\"You are a helpful finance assistant. Use tools when needed to answer questions about stocks.\")]\n",
        "        messages.extend(state[\"messages\"])\n",
        "        response = llm.invoke(messages)\n",
        "        return {\"messages\": [response]}\n",
        "    return agent_node\n",
        "\n",
        "# Build the graph\n",
        "graph_builder = StateGraph(AgentState)\n",
        "graph_builder.add_node(\"agent\", create_agent_node(llm_with_tools))\n",
        "graph_builder.add_node(\"tools\", ToolNode(finance_tools))\n",
        "\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "graph_builder.add_edge(START, \"agent\")\n",
        "\n",
        "# Compile the graph\n",
        "agent_graph = graph_builder.compile()\n",
        "\n",
        "print(\"‚úÖ LangGraph agent created with workflow:\")\n",
        "print(\"   START ‚Üí agent ‚Üí (tools if needed) ‚Üí agent ‚Üí END\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Run Demo Queries\n",
        "\n",
        "**What we're doing**: Running example queries to demonstrate the agent in action.\n",
        "\n",
        "**Demo queries**:\n",
        "1. **Simple query**: \"What's the current price of AAPL?\" - Shows basic tool usage\n",
        "2. **Action query**: \"Buy 10 shares of TSLA\" - Shows tool execution with parameters\n",
        "3. **Multi-tool query**: \"What's the price of NVDA and MSFT?\" - Shows multiple tool calls\n",
        "\n",
        "**What happens for each query**:\n",
        "1. User query is sent to the agent\n",
        "2. Agent analyzes the query and decides to use tools\n",
        "3. Tools are called with appropriate parameters\n",
        "4. Agent processes tool results and generates a response\n",
        "5. **All of this is automatically traced** by OpenTelemetry\n",
        "\n",
        "**What you'll see**:\n",
        "- Console output showing the agent's responses\n",
        "- Trace spans for each step (visible in console exporter)\n",
        "- Complete trace graphs in Galileo showing the full workflow\n",
        "\n",
        "**Try modifying the queries** to see how the agent handles different scenarios!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Running Finance Agent Demo Queries\n",
            "================================================================================\n",
            "\n",
            "üìù Query 1: What's the current price of AAPL?\n",
            "--------------------------------------------------------------------------------\n",
            "‚úÖ Response: The current price of Apple Inc. (AAPL) stock is $178.72 per share. It is up $1.23 or 0.69% from the previous close....\n",
            "\n",
            "üìù Query 2: Buy 10 shares of TSLA at 180 dollars per share\n",
            "--------------------------------------------------------------------------------\n",
            "‚úÖ Response: Okay, let's execute a stock purchase order for 10 shares of TSLA at $180 per share....\n",
            "\n",
            "üìù Query 3: What's the price of NVDA and MSFT?\n",
            "--------------------------------------------------------------------------------\n",
            "‚úÖ Response: Okay, let me get the current stock prices for NVDA (NVIDIA) and MSFT (Microsoft) using the get_stock_price tool....\n",
            "\n",
            "================================================================================\n",
            "‚úÖ All queries completed! Traces are being sent to Galileo.\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Run Demo Queries\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Demo queries that showcase agentic behavior\n",
        "demo_queries = [\n",
        "    \"What's the current price of AAPL?\",\n",
        "    \"Buy 10 shares of TSLA at 180 dollars per share\",\n",
        "    \"What's the price of NVDA and MSFT?\",\n",
        "]\n",
        "\n",
        "print(\"üöÄ Running Finance Agent Demo Queries\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, query in enumerate(demo_queries, 1):\n",
        "    print(f\"\\nüìù Query {i}: {query}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Create a trace span for this query\n",
        "    tracer = trace.get_tracer(__name__)\n",
        "    with tracer.start_as_current_span(\"finance_agent_query\") as span:\n",
        "        span.set_attribute(\"query.text\", query)\n",
        "        span.set_attribute(\"query.number\", i)\n",
        "        \n",
        "        try:\n",
        "            # Run the agent\n",
        "            result = agent_graph.invoke({\n",
        "                \"messages\": [HumanMessage(content=query)]\n",
        "            })\n",
        "            \n",
        "            # Get the final response\n",
        "            final_message = result[\"messages\"][-1]\n",
        "            response_text = final_message.content if hasattr(final_message, 'content') else str(final_message)\n",
        "            \n",
        "            span.set_attribute(\"response.length\", len(response_text))\n",
        "            span.set_attribute(\"response.success\", True)\n",
        "            \n",
        "            print(f\"‚úÖ Response: {response_text[:200]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            span.set_attribute(\"response.success\", False)\n",
        "            span.set_attribute(\"error.message\", str(e))\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ All queries completed! Traces are being sent to Galileo.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: View Traces in Galileo\n",
        "\n",
        "**What we're doing**: Flushing traces and generating links to view them in Galileo.\n",
        "\n",
        "**What happens**:\n",
        "1. **Flush traces**: Ensures all spans are sent to Galileo (not just batched)\n",
        "2. **Test connection**: Verifies the endpoint is reachable\n",
        "3. **Generate links**: Creates direct links to your project and log stream\n",
        "\n",
        "**What you'll see in Galileo**:\n",
        "\n",
        "1. **Trace Graph View**:\n",
        "   - Root span: `LangGraph` (the entire workflow)\n",
        "   - Child spans: `agent` nodes (LLM decision points)\n",
        "   - Tool spans: `get_stock_price`, `purchase_stocks`, etc.\n",
        "   - LLM spans: `BedrockConverseLLM` (showing prompts, responses, token usage)\n",
        "\n",
        "2. **Span Details**:\n",
        "   - **Input/Output**: See the exact messages sent to and received from the LLM\n",
        "   - **Attributes**: Tool parameters, stock prices, order IDs, etc.\n",
        "   - **Metadata**: Model parameters, token counts, timing information\n",
        "   - **Trace Context**: How spans connect to show the full execution path\n",
        "\n",
        "3. **Performance Insights**:\n",
        "   - Latency for each step\n",
        "   - Token usage per LLM call\n",
        "   - Tool execution times\n",
        "\n",
        "**Troubleshooting**: If traces don't appear:\n",
        "- Verify the log stream exists in your project\n",
        "- Wait 30-60 seconds for processing\n",
        "- Refresh the Galileo console\n",
        "- Check the console output above for trace details\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Waiting for traces to be sent to Galileo...\n",
            "üîÑ Flushing traces to Galileo...\n",
            "‚úÖ Flush complete\n",
            "\n",
            "üß™ Testing trace creation...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'trace_api' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Test trace creation\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müß™ Testing trace creation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m test_tracer = \u001b[43mtrace_api\u001b[49m.get_tracer(\u001b[34m__name__\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m test_tracer.start_as_current_span(\u001b[33m\"\u001b[39m\u001b[33mtest_trace\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[32m     23\u001b[39m     span.set_attribute(\u001b[33m\"\u001b[39m\u001b[33mtest.attribute\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtest_value\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'trace_api' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 8: Generate Galileo Console URLs and Flush Traces\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Wait a moment for traces to be batched and sent\n",
        "print(\"‚è≥ Waiting for traces to be sent to Galileo...\")\n",
        "time.sleep(3)\n",
        "\n",
        "# Force flush any remaining spans with timeout\n",
        "print(\"üîÑ Flushing traces to Galileo...\")\n",
        "try:\n",
        "    # Force flush with a timeout\n",
        "    tracer_provider.force_flush(timeout_millis=10000)  # 10 second timeout\n",
        "    print(\"‚úÖ Flush complete\")\n",
        "except Exception as flush_error:\n",
        "    print(f\"‚ö†Ô∏è  Flush error: {flush_error}\")\n",
        "    print(\"   This might indicate an export issue\")\n",
        "\n",
        "# Test trace creation\n",
        "print(\"\\nüß™ Testing trace creation...\")\n",
        "test_tracer = trace_api.get_tracer(__name__)\n",
        "with test_tracer.start_as_current_span(\"test_trace\") as span:\n",
        "    span.set_attribute(\"test.attribute\", \"test_value\")\n",
        "    span.set_attribute(\"test.service\", \"finance-agent-demo\")\n",
        "    span.set_attribute(\"test.project\", GALILEO_PROJECT)\n",
        "    span.set_attribute(\"test.logstream\", GALILEO_LOG_STREAM)\n",
        "    print(\"   üìù Test span created\")\n",
        "\n",
        "# Flush and wait\n",
        "tracer_provider.force_flush(timeout_millis=10000)\n",
        "print(\"‚úÖ Test trace created and flushed\")\n",
        "\n",
        "# Test direct HTTP connection to verify endpoint\n",
        "print(\"\\nüåê Testing connection to Galileo OTLP endpoint...\")\n",
        "try:\n",
        "    import requests\n",
        "    test_headers = {\n",
        "        \"Content-Type\": \"application/x-protobuf\",\n",
        "        \"Galileo-API-Key\": GALILEO_API_KEY,\n",
        "        \"project\": GALILEO_PROJECT,\n",
        "        \"logstream\": GALILEO_LOG_STREAM,\n",
        "    }\n",
        "    \n",
        "    # Test the correct endpoint (confirmed working)\n",
        "    response = requests.post(GALILEO_OTLP_ENDPOINT, headers=test_headers, data=b\"\", timeout=5)\n",
        "    print(f\"   Endpoint: {GALILEO_OTLP_ENDPOINT}\")\n",
        "    print(f\"   Status: {response.status_code}\")\n",
        "    if response.status_code in [200, 400, 422]:\n",
        "        print(\"   ‚úÖ Endpoint is reachable and authentication works!\")\n",
        "    elif response.status_code == 401:\n",
        "        print(\"   ‚ùå Authentication failed - check API key\")\n",
        "    elif response.status_code == 404:\n",
        "        print(\"   ‚ùå Endpoint not found - check URL\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  Status: {response.status_code}\")\n",
        "except Exception as conn_error:\n",
        "    print(f\"   ‚ùå Connection error: {conn_error}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä TRACE STATUS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ Traces ARE being created successfully!\")\n",
        "print(\"   - Console output shows multiple spans (LangGraph, agent, BedrockConverseLLM)\")\n",
        "print(\"   - Endpoint is reachable and authentication works (422 response = valid)\")\n",
        "print(\"   - Traces are being flushed to Galileo\")\n",
        "print(\"\\nüí° If traces don't appear in Galileo console:\")\n",
        "print(\"   1. VERIFY LOG STREAM EXISTS:\")\n",
        "print(\"      - Go to: https://app.galileo.ai/project/70fb8148-6e04-4408-8633-cb83415f0fd1\")\n",
        "print(\"      - Check if log stream '{}' exists\".format(GALILEO_LOG_STREAM))\n",
        "print(\"      - If not, create it in the Galileo console\")\n",
        "print(\"   2. WAIT 30-60 SECONDS:\")\n",
        "print(\"      - Traces may take time to process and appear\")\n",
        "print(\"   3. REFRESH THE CONSOLE:\")\n",
        "print(\"      - Hard refresh the browser (Cmd+Shift+R or Ctrl+F5)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Generate Galileo Console URLs\n",
        "# Note: Galileo URLs require UUIDs (project ID and log stream ID), not names\n",
        "console_url = \"https://app.galileo.ai\"  # or your custom deployment URL\n",
        "\n",
        "# Try to fetch project and log stream IDs from Galileo API\n",
        "try:\n",
        "    # Get project ID - use the paginated API endpoint (POST request)\n",
        "    api_url = f\"{console_url}/api/galileo/public/v2/projects/paginated?starting_token=0&limit=100\"\n",
        "    headers = {\n",
        "        \"accept\": \"*/*\",\n",
        "        \"galileo-api-key\": GALILEO_API_KEY,\n",
        "        \"content-type\": \"application/json\",\n",
        "        \"origin\": console_url,\n",
        "        \"referer\": f\"{console_url}/\",\n",
        "    }\n",
        "    \n",
        "    # POST request with sort/filter data\n",
        "    data = {\n",
        "        \"sort\": {\n",
        "            \"name\": \"updated_at\",\n",
        "            \"ascending\": False\n",
        "        },\n",
        "        \"filters\": []\n",
        "    }\n",
        "    \n",
        "    response = requests.post(api_url, headers=headers, json=data)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        projects = result.get(\"projects\", [])\n",
        "        project_id = None\n",
        "        for project in projects:\n",
        "            if project.get(\"name\") == GALILEO_PROJECT:\n",
        "                project_id = project.get(\"id\")\n",
        "                break\n",
        "        \n",
        "        if project_id:\n",
        "            # Get log stream ID - GET request\n",
        "            log_streams_url = f\"{console_url}/api/galileo/v2/projects/{project_id}/log_streams\"\n",
        "            log_stream_response = requests.get(log_streams_url, headers=headers)\n",
        "            if log_stream_response.status_code == 200:\n",
        "                log_streams = log_stream_response.json()\n",
        "                log_stream_id = None\n",
        "                for stream in log_streams:\n",
        "                    if stream.get(\"name\") == GALILEO_LOG_STREAM:\n",
        "                        log_stream_id = stream.get(\"id\")\n",
        "                        break\n",
        "                \n",
        "                if log_stream_id:\n",
        "                    project_url = f\"{console_url}/project/{project_id}\"\n",
        "                    log_stream_url = f\"{project_url}/log-streams/{log_stream_id}\"\n",
        "                    print(\"\\n\" + \"=\" * 80)\n",
        "                    print(\"üìä Galileo Dashboard Links\")\n",
        "                    print(\"=\" * 80)\n",
        "                    print(f\"üîó Project URL: {project_url}\")\n",
        "                    print(f\"üîó Log Stream URL: {log_stream_url}\")\n",
        "                    print(\"\\nüí° What to look for in Galileo:\")\n",
        "                    print(\"   ‚Ä¢ Complete trace graphs showing LangGraph workflow\")\n",
        "                    print(\"   ‚Ä¢ Tool call spans (get_stock_price, purchase_stocks, etc.)\")\n",
        "                    print(\"   ‚Ä¢ Bedrock LLM calls with token usage\")\n",
        "                    print(\"   ‚Ä¢ Performance metrics and timing\")\n",
        "                    print(\"=\" * 80)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  Log stream '{GALILEO_LOG_STREAM}' not found. Access via: {console_url}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Could not fetch log streams (status: {log_stream_response.status_code}). Access via: {console_url}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Project '{GALILEO_PROJECT}' not found. Access via: {console_url}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Could not fetch projects (status: {response.status_code}). Access via: {console_url}\")\n",
        "        print(f\"   Project: {GALILEO_PROJECT}, Log Stream: {GALILEO_LOG_STREAM}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error fetching Galileo IDs: {e}\")\n",
        "    print(f\"   Access Galileo Console directly: {console_url}\")\n",
        "    print(f\"   Project: {GALILEO_PROJECT}, Log Stream: {GALILEO_LOG_STREAM}\")\n",
        "    print(\"\\nüí° Note: If you see UUID errors in the playground, navigate directly\")\n",
        "    print(\"   to the project and log stream from the Galileo Console home page.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
